---
title: "DATA607 Project III - Teamwork"
authors: "Koohyar P, Anthony C, Victor T, & James N"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

While the presidential election season is in full swing, we decided to
explore polling data sources that exist online. There are several
individual sources that could be found online; however, the website
RealClear Politics is a location that gathers, summarizes, and presents
the results of the various polls in one location. It should be noted
that, while this website is good of a summary view, the underlying
polling data must be extracted from the various polling sources (if
available) for further review and analysis. The polling sources include
Emerson College, The Economist Magazine, The New York Times/Sienna
College, CBS News, and many others. Some sources are free, while others
incur a fee. It should be noted that the polls tend to discriminate
between “Registered Voters” (RV) and “Likely Voters” (LV), and the
common belief is the LV are better more indicative of election results.
However, a Berkley Haas Study in 2020 reported that while the polls
reached a 95% confidence level for statistical reporting, the actual
election results only matched with the polls 60% of the time.

## Data Sources

We are currently in discussion to identify the data sources for
analysis, and the type of analysis we wish to discuss. The sources are
varied and include tables on websites, attached PDF documents, and CSV
files. Some will require us prepare the data through another platform
before we are able to evaluate and analyze the data. This also needs to
include a matching/pairing of questions and response on polls to insure
equivalency of the questions. Data that has currently been identified
include The New York Times/Sienna, Roanoke College, and Emerson College
Polls.

## Code Initialization

Here I load the required libraries and ensure all the required packages
are installed before running the following blocks of codes.

```{r Code_initialization, echo=FALSE, message=FALSE}

required_packages <- c("RSQLite","devtools","tidyverse","DBI","dplyr","odbc","openintro","ggplot2","psych","reshape2","knitr","markdown","shiny","R.rsp","fivethirtyeight","RCurl", "stringr","readr","glue","data.table", "hflights", "jsonlite", "rjson", "XML", "xml2", "rvest", "readxl", "openxlsx", "httr") # Specify packages

not_installed <- required_packages[!(required_packages %in% installed.packages()[ , "Package"])]# Extract not installed packages
if(length(not_installed)==0){
  print("All required packages are installed")
} else {
  print(paste(length(not_installed), "package(s) had to be installed.")) # print the list of packages that need to be installed
  install.packages(not_installed)
}

# define different paths to load the files 
library(dplyr)
library(tidyverse)
library(readxl)
library(rvest)
library(knitr)
library(openxlsx)
library(httr)
library(jsonlite)

#surpass the error message for dplyr to not show the masking
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(jsonlite))


```

## Load files from GitHub (KP)

All our files are stored in the GitHub `Data/*` directory for
productivity and collaboration. In this section, I verify the list of
files in the Data folder and then load them all into R. All files are in
CSV format and are readily accessible by RStudio. However, since they
originate from different sources, we must first tidy, clean, and
organize them.

```{r load_files, echo=FALSE}

# GitHub raw URL
GitHub_raw <- "https://raw.githubusercontent.com/kohyarp/DATA607_Project3/main/Data"
CSV_path <- "/NYT_Sienna%20Poll_table_1.csv"
CSV_path <- paste0(GitHub_raw , CSV_path) 
#read CSV file to the path to RStudio

Read_csv <- read.csv(CSV_path,check.names = TRUE,
                     na.strings = "NA", dec = ".", quote = "\"",
                     header = FALSE,
                     encoding = "UTF-8",
                     blank.lines.skip = TRUE)

#Read_csv <- read.csv("https://raw.githubusercontent.com/kohyarp/DATA607_Project3/main/Data/NYT_Sienna%20Poll_table_1.csv")


# Send a GET request to the GitHub raw URL
#GitHub_raw <- "https://raw.githubusercontent.com/kohyarp/DATA607_Project3/main"


repository_url <- "https://api.github.com/repos/kohyarp/DATA607_Project3/contents/Data"

#Load all the relavant tile type from GitHub_raw

GitHub_file_type <-  c("csv", "CSV", "txt", "TXT")  #Different types of file's extension to be loaded 

GitHub_Get_file <- function(repository_url = "https://api.github.com/repos/kohyarp/DATA607_Project3/contents/Data" , GitHub_file_type = c("csv","txt")) {
    
  # Send a GET request to the GitHub API
  response <- GET(repository_url)
  #response <- GET(GitHub_raw)
  #(response)
  # Extract content from the response
  #content <- content(response, "text")
  
  # Check if request was successful
  if (http_type(response) == "application/json") {
  # Parse JSON response
  content <- content(response, as = "text")
  file_list <- fromJSON(content)
  
  # Extract file names from the response
  file_names <- file_list$name
  #file_names <- sapply(file_list, function(x) x$name)
  
  # Filter out directories and unwanted files (like '..' and '.')
  file_names <- file_names[file_names != ".." & file_names != "."]
  # Replace spaces with %20 in file names
  file_names <- URLencode(file_names)
  
  # Print the list of file names
  #print(file_names)
  # Initialize list to store files by type
  files_by_type <- list()
  
  #GitHub API has error loading file from API but laod it correctly from Raw and following code is to help it, and I replaced it with raw to work. 
  repository_url_raw <-  gsub("^https://api\\.github\\.com/repos", 
                          "https://raw.githubusercontent.com", repository_url)
   repository_url_raw <-  gsub("contents", "main", repository_url_raw)
  
    # Iterate through file names
    for (file in file_names) {
      for (type in GitHub_file_type) {
        if (grepl(paste0("\\.", type, "$"), file)) {
          file_url <- paste0(repository_url_raw, "/", file)  # Construct full URL using repository_url
          files_by_type[[type]] <- c(files_by_type[[type]], file_url)  # Add file to list for its type
          message("File", file, "with type", type, "has been identified in the repository.")
        }
      }
    }

    return(files_by_type)
  } else {
    # Print error message if request was not successful
    print("Error: Unable to fetch file list from GitHub.")
    return(NULL)  # Return NULL in case of error
  }

}


#run the function and create a list of file for the different type of file to be loaded later
Github_file_list <- GitHub_Get_file (repository_url = repository_url , GitHub_file_type = GitHub_file_type)


#defien a funtion that get a list of csv fiels and laod them all from GitHub public library 
GitHub_CSV_load <- function(repository_url = "https://api.github.com/repos/kohyarp/DATA607_Project3/contents/Data") {
  # GitHub repository URL
  
  # Send a GET request to the GitHub API
  response <- GET(repository_url)
  #response <- GET(GitHub_raw)
  #(response)
  # Extract content from the response
  #content <- content(response, "text")
  
  # Check if request was successful
  if (http_type(response) == "application/json") {
  # Parse JSON response
  content <- content(response, as = "text")
  file_list <- fromJSON(content)
  
  # Extract file names from the response
  file_names <- file_list$name
  #file_names <- sapply(file_list, function(x) x$name)
  
  # Filter out directories and unwanted files (like '..' and '.')
  file_names <- file_names[file_names != ".." & file_names != "."]
  # Replace spaces with %20 in file names
  file_names <- URLencode(file_names)
  
  # Print the list of file names
  print(file_names)
} else {
  # Print error message if request was not successful
  print("Error: Unable to fetch file list from GitHub.")
  return(NULL)  # Return NULL in case of error
}
  # Parse the content to extract file names
  #file_names <- gsub(".*<a href=\"([^\"]*)\".*", "\\1", content)
  # Filter out directories and unwanted files (like '..' and '.')
  #file_names <- file_names[file_names != ".." & file_names != "."]
  # Initialize list to load files
  loaded_file_list <- list()
  
  # Counter to keep track of loaded files
  file_counter <- 0
  temp <- data.frame(file_names)
  
  # Loop through the file names, download, and load them into R
  for (file in file_names) {
  # Check if the file has .csv or .CSV extension
  if (grepl("\\.csv$|\\.CSV$", file)) {
    # Construct the full URL for each file
    file_url <- paste0(GitHub_raw, "/", file)
    
    # Download the file
    download.file(file_url, destfile = file, mode = "wb")
    
    # Load the file into R
    data <- tryCatch(
      read.csv(file_url,check.names = TRUE,
                     na.strings = "NA", dec = ".", quote = "\"",
                     header = FALSE,
                     encoding = "UTF-8",
                     blank.lines.skip = TRUE),
      
#      read.csv(file, check.names = TRUE, na.strings = "", dec = ".", quote = "\""),
      error = function(e) {
        message("Error loading file:", conditionMessage(e))
        NULL
      }
    )
    
    # Check if the file is successfully loaded
    if (!is.null(data)) {
      # Increment the file counter
      file_counter <- file_counter + 1
      
      # Store file and its data in the loaded_file_list
      loaded_file_list[[file_counter]] <- list(file = file, data = data)
      
      # Print message indicating the file has been loaded
      message("File", file, "has been loaded into R.")
    }
  }
  }
  return(loaded_file_list)
  }

temp_files <- GitHub_CSV_load()

GitHub_load_all <- function(file_list) {

  # Initialize lists to store loaded data
  loaded_csv_data <- list()
  loaded_txt_data <- list()

  # Loop through each type in the file list
  for (type in names(file_list)) {
    # Check if type is either "csv" or "txt"
    if (type %in% c("csv", "txt")) {
      for (file_url in file_list[[type]]) {
        # Download file content using tryCatch for error handling
        data <- tryCatch({
          if (type == "csv") {
            read.csv(file_url,check.names = TRUE,
                     na.strings = "NA", dec = ".", quote = "\"",
                     header = FALSE,
                     encoding = "UTF-8",
                     blank.lines.skip = TRUE)
          } else {
            readLines(file_url)
          }
        }, error = function(e) {
          message("Error loading file:", file_url, ":", paste(e))
          return(NULL)  # Return NULL on error for the specific file
        })

        # Check if data was loaded successfully (not NULL)
        if (!is.null(data)) {
          # Store data in the appropriate list based on type
          if (type == "csv") {
            loaded_csv_data[[tail(file_url, 1)]] <- data  # Use filename as key
          } else {
            loaded_txt_data[[tail(file_url, 1)]] <- data  # Use filename as key
          }
          message("File", tail(file_url, 1), "with type", type, "has been loaded.")
        }
      }
    }
  }

  # Combine loaded data into a single list with type information
  all_loaded_files <- list(
    csv_data = loaded_csv_data,
    txt_data = loaded_txt_data
  )

  return(all_loaded_files)
}


GitHub_loaded_list <- GitHub_load_all(Github_file_list)

temp_csv_file <- temp_files_contents [[1]][[1]]


```

## Tidying and data cleanup (KP)

All files are loaded into `GitHub_loaded_list` in those sections fo the
code, we clean up the data and change them to long format to be bale to
do further analyses on them. The first step is to go through the data
stored in each dataframe and attempt to understand their structure.

```{r tidying, echo=TRUE}

# go though the list and extact each data.frame to a tibble or simialr thing for csv files 
#csv file has a better an already devided into a dataframe, the issue with the data is that it is not long and sorrectly strcutured. 

for (i in length(GitHub_loaded_list[["csv_data"]])){
  
  
}

```

### Data Import (AC)

<https://ballotpedia.org/Super_Tuesday_primaries,_2024>

```{r import}
url <- "https://ballotpedia.org/Super_Tuesday_primaries,_2024"
webpage <- read_html(url)
st_table <- html_nodes(webpage, '.portal-section')
table_names <- c("Alabama", "Alaska", "American Somoa", "Arkansas", "California", "Colorado", "Iowa", "Maine", "Massachusetts", "Minnesota", "North_Carolina", "Oklahoma", "Tennessee", "Texas", "Utah", "Vermont", "Virginia")
table_frames <- list()
for (i in seq_along(st_table)) {
  table_data <- html_table(st_table[[i]])
  table_name <- table_names[i]
  table_frames[[table_name]] <- table_data
}
# Sample Data Frame
table_frames$Alabama

```

### Remove Extraneous Rows (AC)

```{r removing rows with 99% reporting or Source}
for (i in seq_along(table_frames)){
  # Filter the rows that contain the character string "% reporting"
  table_frames[[i]] <- table_frames[[i]][!grepl("% reporting", table_frames[[i]]$X2), ]
}
# Loop through each data frame removing the row that contains "Source" in column X2
for (i in seq_along(table_frames)){
  # Filter the rows that contain the character string "Source"
  table_frames[[i]] <- table_frames[[i]][!grepl("Source", table_frames[[i]]$X2), ]
}
# for (i in seq_along(table_frames)) {
#  print(table_frames[[i]])
#}
table_frames[[1]]
```

### Adding Columns for State and Party affiliation (AC)

```{r adding columns for state and party affilation, echo=FALSE}
add_columns <- function(df){
  df |> mutate(X6 = if_else(X2 == "Candidate", "state", NA),
               X7 = if_else(X2 == "Candidate", "party", NA))
}
table_frames <- map(table_frames, add_columns)

table_frames[[1]]
```

### Extracting of Data Frames from List for Super Tuesday States (AC)

Selecting the proper columns, dropping the unnecessary ones. This also
places "Democrat into the Party Column for ALL candidates, which will
fixed in a later section to include Republican in the correct candidate.
Pulls the individual state data frames out of the list to be worked on
individually.

```{r super tuesday states, echo=FALSE}
alabama <- table_frames[[1]] |> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "alabama", X6))|> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
arkansas <- table_frames[[4]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "arkansas", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
california <- table_frames[[5]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "california", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
colorado <- table_frames[[6]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "colorado", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
maine <- table_frames[[8]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "maine", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
massachusets <- table_frames[[9]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "massachusets", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
minnesota <- table_frames[[10]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "minnesota", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
north_carolina <- table_frames[[11]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "north_carolina", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
oklahoma <- table_frames[[12]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "oklahoma", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
tennessee <- table_frames[[13]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "tennnessee", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
texas<- table_frames[[14]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "texas", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
utah <- table_frames[[15]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "utah", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
vermont <- table_frames[[16]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "vermont", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))
virginia <- table_frames[[17]]|> select(!X1&!X5) |> mutate(X6 = ifelse(is.na(X6), "virginia", X6)) |> mutate(X7 = ifelse(is.na(X7), "Democrat", X7))

alabama
```

### Substituting Republican in the rows (AC)

This uses a filter that identifies the second occurrence in the column
X2 that separates republican candidates from the democratic candidates,
and then replaces "Democrat" with "Republican". This is done for each
individual data frame for the Super Tuesday states.

```{r repubilcan candidates, echo=FALSE}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- alabama$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  alabama <- alabama %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- arkansas$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  arkansas <- arkansas %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- california$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  california <- california %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- colorado$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  colorado <- colorado %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- maine$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  maine <- maine %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- massachusets$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  massachusets <- massachusets %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- minnesota$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  minnesota <- minnesota %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- north_carolina$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  north_carolina <- north_carolina %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- oklahoma$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  oklahoma <- oklahoma %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- tennessee$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  tennessee <- tennessee %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- texas$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  texas <- texas %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- utah$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  utah <- utah %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- vermont$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  vermont <- vermont %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
# Find the index of the second occurrence of "Candidate" in column X2
soi <- virginia$X2 %>%
  grep("Candidate", ., fixed = TRUE) %>%
  .[2]
# Replace values in column X7 after the second occurrence of "Candidate"
if (!is.na(soi)) {
  virginia <- virginia %>%
    mutate(X7 = ifelse(row_number() > soi, "Republican", X7))
}
alabama
```

### Removing Candidate Rows (AC)

In this section, we remove the "candidate" row from each of the data
frames and rename the column variable names from X to what they actually
represent. We also address the accidental import of California
candidates not running for the national office of president because they
were placed in the same table on the website. Those candidates had their
party affiliation and percent_vote values changes to NA.

```{r removing candidate rows by filter, echo=FALSE, warning=FALSE}
alabama <- alabama |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
arkansas <-  arkansas |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
california <-  california |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
# California Cleanup. Five candidates brought over that were not running for president.
california[18:23, "party"] <- "NA"
california[18:23, "percent_vote"] <- "NA"
colorado <-  colorado |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
maine <-  maine |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
massachusets <-  massachusets |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
minnesota <-  minnesota |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
north_carolina <-  north_carolina |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
oklahoma <-  oklahoma |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
tennessee <- tennessee |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
texas <-  texas |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
utah <-  utah|> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
vermont <-  vermont |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
virginia <- virginia |> filter(!X2=="Candidate") |> rename(candidate=X2, percent_vote=X3, num_vote=X4, state=X6, party=X7)
california |> kbl() |> kable_classic_2(full_width=F, font_size=12)

```

### Combining the Entire Data Frame (AC)

In this section we combine all the Super Tuesday state data frames into
one tidy data frame containing five (5) variables and 205 rows. Once in,
we will remove all the commas in the vote counts so we can use the data
as numeric.

```{r Combine}
# The Combine
super_tuesday_combine <- bind_rows(alabama, arkansas, california, colorado, maine, massachusets, minnesota, north_carolina, oklahoma, tennessee, texas, utah, vermont, virginia)

# Remove the commas
super_tuesday_combine$num_vote <- gsub(",", "", super_tuesday_combine$num_vote)
super_tuesday_combine
```

### Total Votes Cast by Party Affiliation and State (AC)

In this section we are going to identify the total number of votes cast
by state, and then grouped by party affiliation.

```{r state and party votes, echo=FALSE}
# Treat the num_vote column as numeric
super_tuesday_combine$num_vote <- as.numeric(super_tuesday_combine$num_vote)


#Group by state total votes
state_total_2024 <- super_tuesday_combine |> group_by(state) |> summarise(state_total=sum(num_vote))

state_dem_total_2024 <- super_tuesday_combine |> group_by(state,party) |> filter(party=="Democrat") |>  summarise(state_party_total_d=sum(num_vote), .groups = "keep")

state_rep_total_2024 <- super_tuesday_combine |> group_by(state, party) |> filter(party=="Republican") |>  summarise(state_party_total_r=sum(num_vote), .groups = "keep")

state_data_2024a <- merge(state_total_2024, state_dem_total_2024, by="state")
state_data_2024b <- merge(state_data_2024a, state_rep_total_2024, by="state")

state_final_2024 <- state_data_2024b |> select(!party.x&!party.y)

state_final_2024
```

### Importing Next Dataset (AC)

In this section we are now going to import the state level data on
regsitered voters and party affiliations. This comes from the
websitehttps://worldpopulationreview.com/state-rankings/registered-voters-by-state.
These figures are from October 2022, but should be accurate enough for
our purposes of comparison with the voting electorate on Super Tuesday
2024. This import was much simpler due to the table structured format of
the website, so the transformation and clean up are going to be much
simpler. The initial import required us to move the first row to column
names and then remove the fist row as an observation.

```{r partisan affilation, echo=FALSE}
url <- "https://worldpopulationreview.com/state-rankings/registered-voters-by-state"
webpage <- read_html(url)
vote <- webpage |> html_table(header = NA, fill = TRUE)
vote_2022 <- html_table(webpage, header = TRUE)[[2]]
#colnames(vote_2022) <- vote_2022[1,]
#vote_2022 <- vote_2022[-1,]
#vote_2022
```

### Tidying and Transforming (AC)

In this section we are going to clean up the data frame a bit. We need
to remove the commas and % in the values so we can do calculations
later. We need to adjust the column names to be more workable, and we
will need to mutate the population numbers to be comparable with the
values in the other data frame (these are in thousands).

```{r tidy and transform, echo=FALSE}
# Column names
new_colnames <- c("state", "reg_voter_num", "perc_vote_pop")
colnames(vote_2022) <- new_colnames

# Remove the commas and convert to numeric
vote_2022$reg_voter_num <- gsub(",", "", vote_2022$reg_voter_num)
vote_2022$reg_voter_num <- as.numeric(vote_2022$reg_voter_num)

# Convert units from per 1000
vote_2022 <- vote_2022 |> mutate(reg_voter_num=reg_voter_num*1000)

# Remove % sign
vote_2022$perc_vote_pop <- gsub("%", "", vote_2022$perc_vote_pop)

# Convert percent
vote_2022 <- vote_2022 |> mutate(perc_vote_pop = as.numeric(perc_vote_pop)/100)

# Convert state to lower case
vote_2022$state <- tolower(vote_2022$state)

```

### Filtering the State Level Data to Only Super Tuesday States (AC)

```{r filter state partisan voter date}
super_states <- state_final_2024$state
vote_2022af <- vote_2022[vote_2022$state %in% super_states, ]

combine_data_frame <- merge(state_final_2024, vote_2022af, by="state")

combine_data_frame

```

## Separaret orphan code (AC)

\*\*\* Split the state form numbers in first column state_split \<-
strsplit(vote_2022$State, "\\[") vote_2022$State \<-
sapply(state_split,"[", 1)

\*\*\* Select the columns we want to work with vote_2022a \<-
select(vote_2022, "State", "Total registrants", "Democratic Party",
"Republican Party", "Independent", "Other parties")

\*\*\* Rename the columns new_colnames \<- c("state", "total_voters",
"democratic_voters", "republican_voters","independent_voters", "others")
colnames(vote_2022a) \<- new_colnames

\*\*\* Remove the commas columns_commas \<- c("total_voters",
"democratic_voters", "republican_voters", "independent_voters",
"others") vote_2022a[columns_commas] \<-
lapply(vote_2022a[columns_commas], function(x) gsub(",", "", x))
vote_2022a
